{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize, WordPunctTokenizer\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works?  It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\"\n",
    "\n",
    "# Sentence tokenizer\n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# Word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "# WordPunct tokenizer\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))\n",
    "\n",
    "#difference here is he deal punctuations as a word.. ex. \" 's \" became \" ' \" & \" s \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize','possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "#Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names),'\\n', '='*68)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word),lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    " from nltk.stem import WordNetLemmatizer\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize','possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names),'\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pos(Part of speach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('out.Just', 'IN'), ('you', 'PRP'), ('and', 'CC'), ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "text=nltk.word_tokenize(\"We are going out.Just you and me\")\n",
    "print(nltk.pos_tag(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing text data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split the input text into chunks, where each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "    \n",
    "    #Iterate through the words and divide them into chunks using the input parameter. The function returns a list:\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "    output.append(' '.join(cur_chunk))\n",
    "    return output\n",
    "\n",
    "#Define the main function and read the input data using the Brown corpus. We will read 12,000 words in this case. You are free to read as many words as you want.\n",
    "if __name__=='__main__':\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "    \n",
    "    # Define the number of words in each chunk\n",
    "    chunk_size = 700\n",
    "    #Divide the input text into chunks and display the output:\n",
    "    chunks = chunker(input_data, chunk_size)\n",
    "    print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print('Chunk', i+1, '==>', chunk[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a category predictor(term frequency and inverse document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2844, 40321)\n",
      "\n",
      "Input: You need to be careful with cars when you are driving on slippery roads \n",
      "Predicted category: Autos\n",
      "\n",
      "Input: A lot of devices can be operated wirelessly \n",
      "Predicted category: Electronics\n",
      "\n",
      "Input: Players need to be careful when they are close to goal posts \n",
      "Predicted category: Hockey\n",
      "\n",
      "Input: Political debates help us understand the perspectives of both sides \n",
      "Predicted category: Politics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the category map\n",
    "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos','rec.sport.hockey': 'Hockey', 'sci.electronics': 'Electronics','sci.med': 'Medicine'}\n",
    "\n",
    "# Get the training dataset\n",
    "training_data = fetch_20newsgroups(subset='train',categories=category_map.keys(), shuffle=True, random_state=5)\n",
    "\n",
    "# Build a count vectorizer and extract term counts\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data:\", train_tc.shape)\n",
    "\n",
    "# Create the tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "\n",
    "# Define test data\n",
    "input_data = ['You need to be careful with cars when you are driving on slippery roads','A lot of devices can be operated wirelessly','Players need to be careful when they are close to goal posts','Political debates help us understand the perspectives of both sides']\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "\n",
    "# Transform input data using count vectorizer\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "\n",
    "# Transform vectorized data using tfidf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "# Predict the output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nInput:', sent, '\\nPredicted category:',category_map[training_data.target_names[category]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a gender identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of end letters: 1\n",
      "Accuracy = 74.7%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> male\n",
      "\n",
      "Number of end letters: 2\n",
      "Accuracy = 78.79%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 3\n",
      "Accuracy = 77.22%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 4\n",
      "Accuracy = 69.98%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 5\n",
      "Accuracy = 64.63%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names\n",
    "\n",
    "\n",
    "# Extract last N letters from the input word\n",
    "# and that will act as our \"feature\"\n",
    "def extract_features(word, N=2):\n",
    "    last_n_letters = word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}\n",
    "\n",
    "male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "data = (male_list + female_list)\n",
    "\n",
    "# Seed the random number generator\n",
    "random.seed(5)\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Create test data\n",
    "input_names = ['Alexander', 'Danielle', 'David', 'Cheryl']\n",
    "\n",
    "# Define the number of samples used for train and test\n",
    "num_train = int(0.8 * len(data))\n",
    "\n",
    "# Iterate through different lengths to compare the accuracy\n",
    "for i in range(1, 6):\n",
    "    print('\\nNumber of end letters:', i)\n",
    "    features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
    "    \n",
    "\n",
    "    #Separate the data into training and testing:\n",
    "    train_data, test_data = features[:num_train], features[num_train:]\n",
    "\n",
    "    #Build a NaiveBayes Classifier using the training data:\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    #Compute the accuracy of the classifier using the inbuilt method available in NLTK:\n",
    "    # Compute the accuracy of the classifier\n",
    "    accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
    "    print('Accuracy = ' + str(accuracy) + '%')\n",
    "\n",
    "\n",
    "    # Predict outputs for input names using the trained classifier model\n",
    "    for name in input_names:\n",
    "        print(name, '==>', classifier.classify(extract_features(name,i)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 15 most informative words:\n",
      "1. outstanding\n",
      "2. insulting\n",
      "3. vulnerable\n",
      "4. ludicrous\n",
      "5. uninvolving\n",
      "6. avoids\n",
      "7. astounding\n",
      "8. fascination\n",
      "9. anna\n",
      "10. animators\n",
      "11. symbol\n",
      "12. darker\n",
      "13. seagal\n",
      "14. affecting\n",
      "15. idiotic\n",
      "\n",
      "Movie review predictions:\n",
      "\n",
      "Review: The costumes in this movie were great\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.59\n",
      "\n",
      "Review: I think the story was terrible and the characters were very weak\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.8\n",
      "\n",
      "Review: People say that the director of the movie is amazing\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.6\n",
      "\n",
      "Review: This is such an idiotic movie. I will not recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.87\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "\n",
    "# Extract features from the input list of words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "\n",
    "# Load the reviews from the corpus\n",
    "fileids_pos = movie_reviews.fileids('pos')\n",
    "fileids_neg = movie_reviews.fileids('neg')\n",
    "\n",
    "# Extract the features from the reviews\n",
    "features_pos = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "features_neg = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "\n",
    "# Define the train and test split (80% and 20%)\n",
    "threshold = 0.8\n",
    "num_pos = int(threshold * len(features_pos))\n",
    "num_neg = int(threshold * len(features_neg))\n",
    "\n",
    "# Create training and training datasets\n",
    "features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "features_test = features_pos[num_pos:] + features_neg[num_neg:]\n",
    "\n",
    "# Print the number of datapoints used\n",
    "print('\\nNumber of training datapoints:', len(features_train))\n",
    "print('Number of test datapoints:', len(features_test))\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(features_train)\n",
    "print('\\nAccuracy of the classifier:', nltk_accuracy(classifier, features_test))\n",
    "\n",
    "N = 15\n",
    "print('\\nTop ' + str(N) + ' most informative words:')\n",
    "for i, item in enumerate(classifier.most_informative_features()):\n",
    "    print(str(i+1) + '. ' + item[0])\n",
    "    if i == N - 1:\n",
    "        break\n",
    "\n",
    "# Test input movie reviews\n",
    "input_reviews = [\n",
    "'The costumes in this movie were great',\n",
    "'I think the story was terrible and the characters were very weak',\n",
    "'People say that the director of the movie is amazing',\n",
    "'This is such an idiotic movie. I will not recommend it to anyone.']\n",
    " \n",
    "print(\"\\nMovie review predictions:\")\n",
    "for review in input_reviews:\n",
    "    print(\"\\nReview:\", review)    \n",
    "    \n",
    "    # Compute the probabilities\n",
    "    probabilities =classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "    # Pick the maximum value\n",
    "    predicted_sentiment = probabilities.max()\n",
    "    \n",
    "    # Print outputs\n",
    "    print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "    print(\"Probability:\",round(probabilities.prob(predicted_sentiment), 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling using Latent DirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 contributing words to each topic:\n",
      "\n",
      "Topic 0\n",
      "\"habit\" ==> 7.9%\n",
      "\"way\" ==> 4.3%\n",
      "\"feel\" ==> 3.1%\n",
      "\"day\" ==> 3.0%\n",
      "\"break\" ==> 3.0%\n",
      "\n",
      "Topic 1\n",
      "\"time\" ==> 4.7%\n",
      "\"take\" ==> 3.6%\n",
      "\"morn\" ==> 3.6%\n",
      "\"habit\" ==> 3.4%\n",
      "\"trigger\" ==> 2.7%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "    return data\n",
    "\n",
    "# Processor function for tokenizing, removing stopwords, and stemming\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Get the list of stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "    \n",
    "    # Remove the stop words\n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    # Perform stemming on the tokenized words\n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "    return tokens_stemmed\n",
    "\n",
    "# Load input data\n",
    "data = load_data('data.txt')\n",
    "\n",
    "# Create a list for sentence tokens\n",
    "tokens = [process(x) for x in data]\n",
    "\n",
    "# Create a dictionary based on the sentence tokens\n",
    "dict_tokens = corpora.Dictionary(tokens)\n",
    "\n",
    "# Create a document-term matrix\n",
    "doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Define the number of topics for the LDA model\n",
    "num_topics = 2\n",
    "\n",
    "# Generate the LDA model\n",
    "ldamodel = models.ldamodel.LdaModel(doc_term_mat,\n",
    "    num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "num_words = 5\n",
    "print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
    "for item in ldamodel.print_topics(num_topics=num_topics,num_words=num_words):\n",
    "    print('\\nTopic', item[0])\n",
    "    # Print the contributing words along with their relative contributions\n",
    "    list_of_strings = item[1].split(' + ')\n",
    "    for text in list_of_strings:\n",
    "        weight = text.split('*')[0]\n",
    "        word = text.split('*')[1]\n",
    "        print(word, '==>', str(round(float(weight) * 100, 2)) + '%')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
